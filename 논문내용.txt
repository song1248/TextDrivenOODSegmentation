Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation
Seungheon Song1 and Jaekoo Lee1∗
Abstract— In autonomous driving and robotics, ensuring road safety and reliable decision-making critically depends on out-of-distribution (OOD) segmentation. While numerous methods have been proposed to detect anomalous objects on the road, leveraging the vision-language space–which provides rich linguistic knowledge–remains an underexplored field. We hypothesize that incorporating these linguistic cues can be especially beneficial in the complex contexts found in real-world autonomous driving scenarios.
To this end, we present a novel approach that trains a Text-Driven OOD Segmentation model to learn a semantically diverse set of objects in the vision-language space. Concretely, our approach combines a vision-language model’s encoder with a transformer decoder, employs Distance-Based OOD prompts located at varying semantic distances from in-distribution (ID) classes, and utilizes OOD Semantic Augmentation for OOD representaitons. By aligning visual and textual information, our approach effectively generalizes to unseen objects and provides robust OOD segmentation in diverse driving environments.
We conduct extensive experiments on publicly available OOD segmentation datasets such as Fishyscapes, Segment-Me-If-You- Can, and Road Anomaly datasets, demonstrating that our approach achieves state-of-the-art performance across both pixel-level and object-level evaluations. This result underscores
1Seungheon Song and Jaekoo Lee are with the College of Computer Science, Kookmin University, Seoul, Republic of Korea
∗Corresponding author: Jaekoo Lee (jaekoo@kookmin.ac.kr)
the potential of vision-language–based OOD segmentation to bolster the safety and reliability of future autonomous driving systems.
I. INTRODUCTION
Semantic segmentation is essential for autonomous driving because it classifies environment elements (e.g., roads, pedestrians, vehicles) at the pixel level. However, out-of-distribution (OOD) objects—unseen during train- ing—frequently appear in real-world driving scenarios. Ex- isting segmentation models often misclassify these OOD objects as in-distribution (ID), increasing the risk of col- lisions. Previous OOD segmentation methods [1], [2], [3], [4], [5], [6] typically rely on vision-only approach (see the top of Figure1(a)) or specialized OOD datasets, but they still struggle to generalize effectively (see Figure 1(b)).
To address these issues, we propose: i) Text-Driven OOD Segmentation: By integrating CLIP’s vision-language mod- eling [7] into Mask2Former [8], our method processes both textual and visual cues for more robust OOD recognition. ii) Distance-Based OOD Prompts: Using WordNet [9], we generate multiple OOD queries based on their semantic distance from ID classes, improving OOD segmentation
 Fig. 1: Overview of limitations in existing OOD segmentation approaches and advantages of the proposed approach. (a) Existing methods often rely solely on visual information (vision-only), whereas our vision-language approach incorporates textual cues in addition to images. (b) By leveraging semantic information from text, our method learns clearer decision boundaries in the joint ID and OOD feature space. (c) Unlike uncertainty- or generation-based methods that use only visual cues, our approach leverages textual knowledge to achieve more reliable OOD scoring.
accuracy in the Mask2Former Transformer decoder. iii) OOD Semantic Augmentation: Instead of inserting external objects (e.g., from COCO [10]), we apply self-attention–based fea- ture adjustments to diversify OOD representations, enabling better handling of unseen anomalies in real-world driving.
By combining a text-driven OOD segmentation, distance- based OOD prompt, and OOD semantic augmentation, our approach delivers a robust solution for OOD segmentation in autonomous driving. Extensive evaluations on multiple datasets confirm its superior performance and generalization compared to existing methods. Overall, our text-driven strat- egy significantly enhances OOD segmentation robustness and marks a promising step toward vision-language–based perception in autonomous driving.